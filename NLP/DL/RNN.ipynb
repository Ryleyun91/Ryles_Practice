{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74ec68f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "595a88ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da7bd1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = [1, 0, 0, 0]\n",
    "e = [0, 1, 0, 0]\n",
    "l = [0, 0, 1, 0]\n",
    "o = [0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "131bd10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = np.array([[h]], dtype=np.float32)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5dce336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample 수(배치), token 수, token의 임베딩\n",
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2741601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data : [[[1. 0. 0. 0.]]] \t shape : (1, 1, 4)\n",
      "output : [[[ 0.31773362 -0.11744198]]] \t shape : (1, 1, 2)\n",
      "states: [[ 0.31773362 -0.11744198]] \t shape: (1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-30 09:27:55.565770: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 2\n",
    "\n",
    "# 마지막 레이어의 units가 2니까 output은 2\n",
    "cell = layers.SimpleRNNCell(units = hidden_size)\n",
    "\n",
    "# return_sequences: bool 형태로 T면 output 전체를 출력하는 것이고, F면 마지막 output만 출력\n",
    "# return_state: bool 형태로 output에 더해 마지막 상태도 반환할지 결정하는 것\n",
    "rnn = layers.RNN(cell, return_sequences = True, return_state = True)\n",
    "output, states = rnn(x_data)\n",
    "\n",
    "print('x_data : {} \\t shape : {}'.format(x_data, x_data.shape))\n",
    "print('output : {} \\t shape : {}'.format(output, output.shape))\n",
    "print('states: {} \\t shape: {}'.format(states, states.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "091dcc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data : [[[1. 0. 0. 0.]]] \t shape : (1, 1, 4)\n",
      "output : [[[0.44843182 0.251574  ]]] \t shape : (1, 1, 2)\n",
      "states: [[0.44843182 0.251574  ]] \t shape: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "# cell 과 rnn 합치기\n",
    "\n",
    "rnn = layers.SimpleRNN(units=hidden_size, return_sequences = True, return_state = True)\n",
    "\n",
    "output, states = rnn(x_data)\n",
    "\n",
    "print('x_data : {} \\t shape : {}'.format(x_data, x_data.shape))\n",
    "print('output : {} \\t shape : {}'.format(output, output.shape))\n",
    "print('states: {} \\t shape: {}'.format(states, states.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcd1a897",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 여러개 넣어보기\n",
    "### shape를 이해하자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "511cf572",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[h,e,l,l,o]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e3a450f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeaef113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data : [[[1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1.]]] \t shape : (1, 5, 4)\n",
      "output : [[[ 0.54152584 -0.00444528]\n",
      "  [-0.24527906  0.8775963 ]\n",
      "  [ 0.81667435 -0.7172945 ]\n",
      "  [-0.39880708  0.19528553]\n",
      "  [ 0.4313137  -0.69072646]]] \t shape : (1, 5, 2)\n",
      "states: [[ 0.4313137  -0.69072646]] \t shape: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "# cell 과 rnn 합치기\n",
    "\n",
    "rnn = layers.SimpleRNN(units=hidden_size, return_sequences = True, return_state = True)\n",
    "\n",
    "output, states = rnn(x_data)\n",
    "\n",
    "print('x_data : {} \\t shape : {}'.format(x_data, x_data.shape))\n",
    "print('output : {} \\t shape : {}'.format(output, output.shape))\n",
    "print('states: {} \\t shape: {}'.format(states, states.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a554779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### batch 사이즈 바꿔보기\n",
    "\n",
    "x_data = np.array([[h, e, l, l, o], [e, o, l, l, l], [l, l, e, e, l]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f40d1a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data : [[[1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1.]]\n",
      "\n",
      " [[0. 1. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0.]]\n",
      "\n",
      " [[0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]]] \t shape : (3, 5, 4)\n",
      "output : [[[-0.12818007  0.04968958]\n",
      "  [-0.12667213 -0.66418386]\n",
      "  [ 0.13962443  0.6124754 ]\n",
      "  [-0.57845664 -0.30636993]\n",
      "  [ 0.4918073   0.28548098]]\n",
      "\n",
      " [[-0.00650635 -0.6992318 ]\n",
      "  [ 0.83611715  0.11154988]\n",
      "  [ 0.17084251 -0.4801117 ]\n",
      "  [ 0.19048181  0.35132104]\n",
      "  [-0.4037878  -0.18411745]]\n",
      "\n",
      " [[-0.2763234   0.18424074]\n",
      "  [-0.5391817   0.27078918]\n",
      "  [-0.5094133  -0.55813867]\n",
      "  [ 0.08890549 -0.11569378]\n",
      "  [-0.13709441  0.1911195 ]]] \t shape : (3, 5, 2)\n",
      "states: [[ 0.4918073   0.28548098]\n",
      " [-0.4037878  -0.18411745]\n",
      " [-0.13709441  0.1911195 ]] \t shape: (3, 2)\n"
     ]
    }
   ],
   "source": [
    "rnn = layers.SimpleRNN(units=hidden_size, return_sequences = True, return_state = True)\n",
    "\n",
    "output, states = rnn(x_data)\n",
    "\n",
    "print('x_data : {} \\t shape : {}'.format(x_data, x_data.shape))\n",
    "print('output : {} \\t shape : {}'.format(output, output.shape))\n",
    "print('states: {} \\t shape: {}'.format(states, states.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d971912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글도 해보자\n",
    "\n",
    "idx2char = ['토', '마', '를', '먹', '자']\n",
    "x_data = [[0, 0, 1, 2, 4, 3]] # 토 토 마 를 자 먹\n",
    "y_data = [[0, 1, 0, 2, 3, 4]] # 토 마 토 를 먹 자\n",
    "\n",
    "input_dim = 5\n",
    "sequence_len = 6\n",
    "learning_rate = 0.1\n",
    "\n",
    "# onehot\n",
    "x_one_hot = tf.keras.utils.to_categorical(x_data, num_classes=input_dim)\n",
    "y_one_hot = tf.keras.utils.to_categorical(y_data, num_classes=input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecaee314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "cell = layers.SimpleRNNCell(units=input_dim, input_shape=(sequence_len, input_dim))\n",
    "\n",
    "model.add(layers.RNN(cell=cell, return_sequences=True, return_state=False, input_shape=(sequence_len, input_dim)))\n",
    "\n",
    "# 모든 타임 스텝에서 출력을 Dense 층에 적용하는 역할을 한다. 쉽게 말해 매 스텝마다 FC가 연결된 것처럼 이해할 수 있다. 각 타임 스텝을 별개의 샘플처럼 다루도록 입력의 크기를 바꾸어 이를 효과적으로 수행한다. \n",
    "model.add(layers.TimeDistributed(layers.Dense(units = input_dim, activation='softmax')))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b51394b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "rnn_1 (RNN)                  (None, 6, 5)              55        \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 6, 5)              30        \n",
      "=================================================================\n",
      "Total params: 85\n",
      "Trainable params: 85\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-30 09:28:41.929976: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 1s 548ms/step - loss: 1.6575 - acc: 0.1667\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3120 - acc: 0.6667\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0190 - acc: 0.8333\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7978 - acc: 1.0000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6424 - acc: 1.0000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4994 - acc: 1.0000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3814 - acc: 1.0000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2826 - acc: 1.0000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2040 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1482 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd72ad4f820>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()\n",
    "model.fit(x_one_hot, y_one_hot, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc642bb",
   "metadata": {},
   "source": [
    "## imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62f9457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ebe943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_data, test_data = imdb['train'], imdb['test']\n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "\n",
    "testing_sentences = []\n",
    "testing_labels= []\n",
    "\n",
    "# numpy array로 바꾼 후 utf-8로 디코딩\n",
    "for s, l in train_data:\n",
    "    training_sentences.append(s.numpy().decode('utf8'))\n",
    "    training_labels.append(l.numpy())\n",
    "\n",
    "for s, l in test_data:\n",
    "    testing_sentences.append(s.numpy().decode('utf8'))\n",
    "    testing_labels.append(l.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68443120",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels = np.array(training_labels)\n",
    "testing_labels = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d571fb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 200\n",
    "max_length = 120\n",
    "trunc_type = 'post'\n",
    "oov_tok = '<oov>'\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded = pad_sequences(sequences, maxlen = max_length, truncating = trunc_type)\n",
    "\n",
    "# testing은 알지 못하는 자료이니 fit_on_text 하면 안 됨!\n",
    "testing_sequence = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequence, maxlen = max_length, truncating = trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a4d0c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? this is the kind of film for a snowy sunday afternoon when the rest of the world can go ahead with its own business as you <oov> into a big arm chair and <oov> for a couple of hours wonderful performances from cher and nicolas cage as always gently row the plot along there are no <oov> to cross no dangerous waters just a warm and witty <oov> through new york life at its best a family film in every sense and one that deserves the praise it received'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "decode_review(padded[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11b238bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 120, 200)          2000000   \n",
      "_________________________________________________________________\n",
      "simple_rnn_8 (SimpleRNN)     (None, 120, 32)           7456      \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 120, 128)          4224      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 120, 1)            129       \n",
      "=================================================================\n",
      "Total params: 2,011,809\n",
      "Trainable params: 2,011,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "                             tf.keras.layers.SimpleRNN(units=32, input_shape=(max_length, embedding_dim), return_sequences=True),\n",
    "                             layers.TimeDistributed(layers.Dense(128, activation='relu')),\n",
    "                            tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32543f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f67108fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "250/250 [==============================] - 14s 53ms/step - loss: 0.5525 - acc: 0.7106 - val_loss: 0.5435 - val_acc: 0.7358\n",
      "Epoch 2/5\n",
      "250/250 [==============================] - 11s 45ms/step - loss: 0.4276 - acc: 0.7986 - val_loss: 0.5245 - val_acc: 0.7302\n",
      "Epoch 3/5\n",
      "250/250 [==============================] - 11s 45ms/step - loss: 0.3716 - acc: 0.8264 - val_loss: 0.5807 - val_acc: 0.7333\n",
      "Epoch 4/5\n",
      "250/250 [==============================] - 11s 44ms/step - loss: 0.3252 - acc: 0.8473 - val_loss: 0.6508 - val_acc: 0.7273\n",
      "Epoch 5/5\n",
      "250/250 [==============================] - 11s 45ms/step - loss: 0.2803 - acc: 0.8678 - val_loss: 0.6335 - val_acc: 0.7273\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "history = model.fit(padded, training_labels, validation_data=(testing_padded, testing_labels),\n",
    "                    epochs=NUM_EPOCHS, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6452afec",
   "metadata": {},
   "source": [
    "## Subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2df14d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c11b4767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n"
     ]
    }
   ],
   "source": [
    "imdb, info = tfds.load('imdb_reviews/subwords8k', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "67424931",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = imdb['train'], imdb['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d50fcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(163,), dtype=int64, numpy=\n",
      "array([  62,   18,   41,  604,  927,   65,    3,  644, 7968,   21,   35,\n",
      "       5096,   36,   11,   43, 2948, 5240,  102,   50,  681, 7862, 1244,\n",
      "          3, 3266,   29,  122,  640,    2,   26,   14,  279,  438,   35,\n",
      "         79,  349,  384,   11, 1991,    3,  492,   79,  122,  188,  117,\n",
      "         33, 4047, 4531,   14,   65, 7968,    8, 1819, 3947,    3,   62,\n",
      "         27,    9,   41,  577, 5044, 2629, 2552, 7193, 7961, 3642,    3,\n",
      "         19,  107, 3903,  225,   85,  198,   72,    1, 1512,  738, 2347,\n",
      "        102, 6245,    8,   85,  308,   79, 6936, 7961,   23, 4981, 8044,\n",
      "          3, 6429, 7961, 1141, 1335, 1848, 4848,   55, 3601, 4217, 8050,\n",
      "          2,    5,   59, 3831, 1484, 8040, 7974,  174, 5773,   22, 5240,\n",
      "        102,   18,  247,   26,    4, 3903, 1612, 3902,  291,   11,    4,\n",
      "         27,   13,   18, 4092, 4008, 7961,    6,  119,  213, 2774,    3,\n",
      "         12,  258, 2306,   13,   91,   29,  171,   52,  229,    2, 1245,\n",
      "       5790,  995, 7968,    8,   52, 2948, 5240, 8039, 7968,    8,   74,\n",
      "       1249,    3,   12,  117, 2438, 1369,  192,   39, 7975])>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-30 10:24:12.523629: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for item in train_data:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0cff531b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the_', ', ', '. ', 'a_', 'and_', 'of_', 'to_', 's_', 'is_', 'br']\n"
     ]
    }
   ],
   "source": [
    "# subword 인코딩해서 가져오기\n",
    "tokenizer = info.features['text'].encoder\n",
    "print(tokenizer.subwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "19e32569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6307, 2327, 4043, 2120, 2, 48, 4249, 4429, 7, 2652, 8050]\n"
     ]
    }
   ],
   "source": [
    "string = 'TensorFlow, from basics to mastery'\n",
    "\n",
    "tokenized_string = tokenizer.encode(string)\n",
    "print(tokenized_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b4f318a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow, from basics to mastery\n"
     ]
    }
   ],
   "source": [
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print(original_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "117dd2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6307 -> Ten\n",
      "2327 -> sor\n",
      "4043 -> Fl\n",
      "2120 -> ow\n",
      "2 -> , \n",
      "48 -> from \n",
      "4249 -> basi\n",
      "4429 -> cs \n",
      "7 -> to \n",
      "2652 -> master\n",
      "8050 -> y\n"
     ]
    }
   ],
   "source": [
    "for token in tokenized_string:\n",
    "    print('{} -> {}'.format(token, tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "759b1972",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = train_data.shuffle(BUFFER_SIZE)\n",
    "# padded_batch? batch_size만큼 패딩 해서 가져오는 것\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(train_dataset))\n",
    "test_dataset = test_data.padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "17bbd97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "max_length = 120\n",
    "model = tf.keras.Sequential([tf.keras.layers.Embedding(tokenizer.vocab_size, embedding_dim),\n",
    "#                              tf.keras.layers.SimpleRNN(32),\n",
    "#                              tf.keras.layers.LSTM(32),\n",
    "                             tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "                             tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "                             tf.keras.layers.Dense(64, activation='relu'),\n",
    "                             tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6b61ec7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, None, 16)          130960    \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, None, 128)         41472     \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 64)                41216     \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 217,873\n",
      "Trainable params: 217,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f73039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 매우 느림 (중단)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['acc'])\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "history = model.fit(train_dataset, validation_data=(test_dataset),\n",
    "                    epochs=NUM_EPOCHS, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7f9865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_' + string])\n",
    "    plt.show()\n",
    "\n",
    "plot_graphs(history, 'acc')\n",
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "96e778bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [\"a\",\"b\",\"c\"]\n",
    "a.index(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17db5e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
