{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1Q9mWDrgMUn"
   },
   "source": [
    "# 9장. 문서 요약 (Text Summarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSZIeetL8Yym"
   },
   "source": [
    "# 9-0 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "executionInfo": {
     "elapsed": 910,
     "status": "error",
     "timestamp": 1625181451708,
     "user": {
      "displayName": "홍경수",
      "photoUrl": "",
      "userId": "10514684854108920109"
     },
     "user_tz": -540
    },
    "id": "_OHg2GEi7ZKI",
    "outputId": "0a1378b7-66be-4efb-de97-1fb8148436af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'과기정통부 22일 유영민 장관 등 참석해 기념행사2021년까지 1516억원 투입 5100여종 데이터 구축민간 클라우드 통한 외부연계체계도. 개방성 강화 국가 차원의 빅데이터 활용 '"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def get_news_by_url(url):\n",
    "    headers = {\"user-agent\" : \"Mozilla/5.0\"}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    text = re.sub(\"\\s*.*오류를 우회하기 위한 함수 추가\\s.*\\s+\",\"\",soup.select_one(\"#articleBodyContents\").text)\n",
    "    text = re.sub(\".{3} \\(\\w+@.+\\).+\\s+\",\"\",text)\n",
    "    text = re.sub(\"[^가-힣A-z0-9\\s\\.\\?\\!]+\",\" \",text)\n",
    "    text = re.sub(\"\\.+\",\". \",text)\n",
    "    text = re.sub(\"\\[.+\\]\", \" \",text)\n",
    "    text = re.sub(\"\\s+\",\" \",text).strip()\n",
    "    \n",
    "    return text\n",
    "    \n",
    "doc = get_news_by_url('https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=105&oid=018&aid=0004430108')\n",
    "doc[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpTxmnbx8v6m"
   },
   "source": [
    "## Mecab 설치 (필요시)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1625181451692,
     "user": {
      "displayName": "홍경수",
      "photoUrl": "",
      "userId": "10514684854108920109"
     },
     "user_tz": -540
    },
    "id": "qsWnZkd78qCA"
   },
   "source": [
    "!sudo apt-get install g++ openjdk-7-jdk # Install Java 1.7+\n",
    "!sudo apt-get install python-dev; pip install konlpy     # Python 2.x\n",
    "!sudo apt-get install python3-dev; pip3 install konlpy   # Python 3.x\n",
    "!sudo apt-get install curl\n",
    "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piKvCjRvgHFE"
   },
   "source": [
    "# 9-1 Luhn Summarizer\n",
    "\n",
    "http://courses.ischool.berkeley.edu/i256/f06/papers/luhn58.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFwBVB3ogZeF"
   },
   "source": [
    "Hans Peter Luhn\n",
    "\n",
    "https://en.wikipedia.org/wiki/Hans_Peter_Luhn\n",
    "\n",
    " Hans Peter Luhn은 공학과 정보과학에서의 개척 작업으로 \"정보 검색의 아버지\"로 알려져 있다. 그는 표제어가 문맥에 포함된 채 배열된 색인(KWIC : keyword-in-context) 개발, 정보 선택 제공(SDI), 완전 텍스트 프로세싱, 자동 발췌(요약), 단어 시소로스의 최초 현대식 사용으로 신뢰를 얻었다. 오늘날 파생된 지식 대부분에는 KWIC 색인이 있으며 과학의 모든 분야에 SDI시스템이 있다. \n",
    "\n",
    "\n",
    " Luhn은 1896년 7월 1일 독일 바르멘에서 태어났다. 아버지가 그 당시 유명한 인쇄업자였으므로, 스위스에서 인쇄업을 배웠다. 어렸을 때무터 그는 창조성이 뛰어난 재능을 보였으며, 기술적 문제, 물리학, 통계학에 관심을 보였다. 그러나 1차 세계대전으로 독일군 통신장교로 복역(1915년1917년)하면서 프랑스 터키 루마니아 불가리아 등지로 옮겨 다녀야만 했다. 1차 대전 이후 그는 스위스의 Gallen 교회 Schweizwrische handels Hochschule로 돌아와 기술, 물리학, 회계분야의 수업을 들었다. 그 후, Luhn은 그리스에서 못다한 공부에 전념했으며, 더블부기기계(Duble-Entry Bookeeping Machine : 카드 대장에 대변기입과 차변 기입을 기록할 수 있는 것)를 발명하였다. 그는 또 Hollerith tabulating/recording machine에 정통했고, 천공카드에서 문자 숫자를 나타내는 장치의 사용을 증가시키게 했다. 1920년부터 1930년까지는 10개의 특허권을 획득하여 그의 탁월한 능력을 보여주었다. 그것들중에 루노메터(Lunometer : 직물의 실길이를 계산하는데 쓰이는 장치)는 지금도 사용되고 있다.\n",
    "\n",
    "\n",
    " 1920년까지 그는 직물 공장에서 일하기 시작했다. 그는 직물 공장의 사업확장을 위해 뉴잉글랜드에서 1924년 미국으로 가게 되었다. 그러나 회사가 곧 파산하였고 Luhn은 직장없이 뉴욕에 남게 되었다. 그는 은행에서 일을 하였고 곧 뉴욕 월스트리트에 소재한 국제어음은행(International Acceptance Bank)에서 재정담당관으로 승진하였다. \n",
    "\n",
    "\n",
    " 1933년 Luhn은 자사인 공학회사 H.P. Luhn & Association을 설립하였고 8년간 자문기술자로 일했다. 1941년 Luhn은 IBM에서 수석 연구기술자로 참여하였고 이후에 정보검색연구 관리자로 일했다. Luhn이 IBM에서 새로운 아이디어를 지속적으로 내놓고 문제를 다르게 접근하여 주목을 받는 동안, 다른 기술자들에게 고차원적인 창조를 하도록 자극하면서 그들의 촉매제 역할을 하여 신뢰를 얻었다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "aborted",
     "timestamp": 1625181451694,
     "user": {
      "displayName": "홍경수",
      "photoUrl": "",
      "userId": "10514684854108920109"
     },
     "user_tz": -540
    },
    "id": "GbuQON2YmWpV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'과기정통부 22일 유영민 장관 등 참석해 기념행사2021년까지 1516억원 투입 5100여종 데이터 구축민간 클라우드 통한 외부연계체계도. 개방성 강화 국가 차원의 빅데이터 활용 시대가 열린다. 새로운 산업 창출과 기존 산업의 변화에 이르는 혁신성장 을 위한 센터가 문을 연다. 10개 분야에 걸쳐 데이터 경제 의 발전을 위한 정부의 청사진을 현실로 구현하는데 앞장선다는 계획이다. 22일 과학기술정보통신부는 서울 중구 대한상공회의소에서 데이터 생태계 조성과 혁신 성장의 기반 마련을 위한 빅데이터 플랫폼 및 센터 출범식 행사를 개최했다. 유영민 과기정통부 장관을 비롯해 노웅래 국회 과학기술정보방송통신위원회 위원장 등 300여명이 참가했다. 10개 분야 100개 센터. 3년간 1516억원 투입이미지 픽사베이빅데이터는 데이터 활용을 통해 혁신성장을 이루자는 문재인 정부의 경제 성장 핵심 요소중 하나다. 문재인 대통령이 직접 올 들어 데이터 활용과 이에 따른 정보보호 보안 에 대한 중요성을 강조하기도 했다. 이런 맥락 속에서 빅데이터센터는 공공과 민간이 협업해 활용도 높은 양질의 데이터를 생산 구축하고 플랫폼은 이를 수집 분석 유통하는 역할을 담당한다. 과기정통부는 분야별 플랫폼 10개소와 이와 연계된 기관별 센터 100개소를 구축하는데 3년간 총 1516억원을 투입할 계획이며 올해 우선 640억원 규모의 사업을 추진하고 있다. 대상 분야는 금융 BC카드 환경 한국수자원공사 문화 한국문화정보원 교통 한국교통연구원 헬스케어 국립암센터 유통 소비 매일방송 통신 KT 중소기업 더존비즈온 지역경제 경기도청 산림 한국임업진흥원 등으로 현재 1차 공모를 통해 72개 빅데이터 센터를 선정했고 다음달 8일까지 2차 공모를 통해 28개를 추가 선정해 총 100개를 지원 운영할 계획이다. 이를 통해 데이터 생태계를 혁신하고 기업의 경쟁력을 제고하는 역할을 수행한다. 주요 활용 전략 사례를 보면 빅데이터 활용을 통해 신 시장 을 창출하는 방안을 담고 있다. 금융 플랫폼의 경우 소상공인 신용평가 고도화 등을 통해 금융 취약 계층 대상 중금리 대출이자를 2 p 절감해 연간 1조원의 신규대출을 창출할 전망이다. 유통 소비와 중소기업 플랫폼은 소상공인이나 중소기업의 폐업률 감소를 문화 플랫폼은 문화 예술 관람률과 생활체육 참여율을 높이는 방안을 모색한다. 의료비 절감 헬스케어 과 기업의 매출 향상을 통한 산업 육성 통신 산림 등도 눈길을 끈다. 과기정통부 제공 2021년까지 5100여종 데이터 구축. AI 알고리즘 제공도센터는 우선 분야별 데이터 부족 문제를 해소하기 위해 올해 말까지 시장 수요가 높은 1400여종 신규 데이터를 생산 구축하고 사업이 완료되는 2021년까지 총 5100여종 양질의 풍부한 데이터를 생산 구축해 시장에 공급할 계획이다. 특히 공공과 민간 사이 데이터 파일형식 등이 달라 호환이 제대로 이뤄지지 못한 문제를 해소하기 위해 개방형 표준을 적용하고 품질관리기준도 마련해 운영한다. 기업들이 실제 활용 가능한 최신 데이터를 확보하는데도 수개월이 소요된다는 문제점을 개선하기 위한 방안도 추진한다. 센터와 플랫폼 간 연계체계에는 민간 클라우드를 기반으로 활용하고 센터에 축적된 데이터도 계속 외부와 개방 공유하며 최신 연속성을 확보한다는 계획이다. 100개 센터에서 수집된 데이터를 융합 분석한 뒤 맞춤형 데이터 제작 등 양질의 데이터로 재생산하고 기업들이 필요로 하는 데이터를 원하는 형태로 즉시 활용할 수 있도록 제공할 계획이다. 다양한 분석 도구는 물론 인공지능 AI 학습 알고리즘도 제공해 이용자가 보다 사용하기 편리한 환경을 제공한다. 이밖에 필요한 데이터를 쉽게 등록하고 검색할 수 있도록 기준을 마련하고 데이터 보유와 관리에 대한 체계 거버넌스 를 논의하는 데이터 얼라이언스 를 구성해 보다 안전하게 이용하는 방안도 마련했다. 유영민 과기정통부 장관은 오늘 출범식은 대한민국이 데이터 강국으로 가기 위한 초석을 놓은 자리 라며 세계 주요국들보다 데이터 경제로 나아가는 발걸음이 다소 늦었지만 빅데이터 플랫폼과 센터를 지렛대로 우리나라의 낙후된 데이터 생태계를 혁신하고 기업의 경쟁력을 한 단계 제고할 수 있도록 정책적 역량을 집중하겠다 고 밝혔다.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "aborted",
     "timestamp": 1625181451695,
     "user": {
      "displayName": "홍경수",
      "photoUrl": "",
      "userId": "10514684854108920109"
     },
     "user_tz": -540
    },
    "id": "7IIA2vf5kbq9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ryleyun/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPM0f2uh5eq0"
   },
   "source": [
    "### 1) 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "aborted",
     "timestamp": 1625181451695,
     "user": {
      "displayName": "홍경수",
      "photoUrl": "",
      "userId": "10514684854108920109"
     },
     "user_tz": -540
    },
    "id": "xRDgb1Ksd3VH"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "# 문장 분리\n",
    "def get_sentences(txt):     \n",
    "    return sent_tokenize(txt)\n",
    "\n",
    "# 토큰화 \n",
    "def get_words(txt):\n",
    "    return [word for word, pos in mecab.pos(txt) if pos[0] == 'N' and len(word)>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsVUL-Bg5sIw"
   },
   "source": [
    "### 2) 중요 단어 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "aborted",
     "timestamp": 1625181451696,
     "user": {
      "displayName": "홍경수",
      "photoUrl": "",
      "userId": "10514684854108920109"
     },
     "user_tz": -540
    },
    "id": "w6X3Kwj_oIRh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'사과', '포도'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어(토큰)의 가중치 계산 및 범위에 포함되는 토큰 식별 \n",
    "def get_keywords(word_list , min_ratio=0.001, max_ratio=0.5) :\n",
    "    # assert는 방어적 프로그래밍 방법으로, true면 넘어가고 아니면 error 발생시킴.\n",
    "    assert (min_ratio < 1 and max_ratio < 1)\n",
    "\n",
    "    # 토큰별로 빈도수 카운팅\n",
    "    count_dict = {}\n",
    "    for word in word_list:\n",
    "#         if word in count_dict.keys():\n",
    "#             count_dict[word] += 1\n",
    "#         else:\n",
    "#             count_dict[word] = 1\n",
    "        \n",
    "    # setdefault를 통해 하나 추가하는데, 만약 기존에 key가 있으면 그냥 넘어감\n",
    "        count_dict.setdefault(word, 0)\n",
    "        count_dict[word] += 1\n",
    "        \n",
    "    # 분석 문서의 총 토큰수 대비 해당 토큰의 빈도 비율   \n",
    "    keywords = set()\n",
    "    for word, cnt in count_dict.items():\n",
    "        word_percentage = cnt/len(word_list)\n",
    "        \n",
    "        # 사전 정의한 비율내에 포함 된 경우 키워드에 추가        \n",
    "        if word_percentage >= min_ratio and word_percentage <= max_ratio:\n",
    "            keywords.add(word)\n",
    "\n",
    "    return keywords\n",
    "\n",
    "get_keywords(['바나나','사과','바나나','바나나','포도'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmEk0zbd5vcx"
   },
   "source": [
    "### 3) 문장 중요도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "aborted",
     "timestamp": 1625181451697,
     "user": {
      "displayName": "홍경수",
      "photoUrl": "",
      "userId": "10514684854108920109"
     },
     "user_tz": -540
    },
    "id": "K-mVXXWYoK5C"
   },
   "outputs": [],
   "source": [
    "# 문장의 가중치 계산\n",
    "def get_sentence_weight (sentence , keywords):\n",
    "    tokens = sentence.split(\" \")\n",
    "    window_start, window_end = 0, -1\n",
    "    \n",
    "    # 문장내에서 윈도 시작 위치 탐색\n",
    "    for i in range(len(tokens)):\n",
    "        # 범위내 속한 키워드가 등장하는 첫번째 위치 계산\n",
    "        if tokens[i] in keywords:\n",
    "            window_start = i\n",
    "            break\n",
    "    \n",
    "    # 문장내에서 윈도 종료 위치 탐색\n",
    "    for i in range(len(tokens) - 1, 0, -1):\n",
    "        # 범위내 속한 키워드가 등장하는 마지막 위치 계산\n",
    "        if tokens[i] in keywords:\n",
    "            window_end = i \n",
    "            break\n",
    "        \n",
    "    \n",
    "    # 윈도의 시작위치가 종료위치보다 큰경우 => 분석할 단어(토큰)가 없는 경우 종료\n",
    "    if window_start > window_end:\n",
    "        return 0\n",
    "        \n",
    "    \n",
    "    # 윈도 크기 계산\n",
    "    window_size = window_end - window_start + 1\n",
    "    \n",
    "    \n",
    "    # 분석 대상 문장 중 범위(0.001 ~ 0.5)에 포함된 토큰 개수 카운팅\n",
    "    keyword_cnt = 0\n",
    "    for w in tokens[window_start : window_start + window_size]:\n",
    "        if w in keywords:\n",
    "            keyword_cnt += 1\n",
    "    \n",
    "    \n",
    "    # (분석 대상 문장 중 범위(0.001 ~ 0.5)에 포함된 토큰 개수)의 제곱 / 윈도사이즈\n",
    "    return keyword_cnt * keyword_cnt / float(window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rufxIxIb5zq9"
   },
   "source": [
    "### 4) 문서 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "aborted",
     "timestamp": 1625181451697,
     "user": {
      "displayName": "홍경수",
      "photoUrl": "",
      "userId": "10514684854108920109"
     },
     "user_tz": -540
    },
    "id": "ZntB_Sx0d832"
   },
   "outputs": [],
   "source": [
    "# 문서 요약\n",
    "def summarize(content, max_no_of_sentences = 10):\n",
    "    \n",
    "    # 단어(토큰) 분리\n",
    "    word_list = get_words(content)\n",
    "    \n",
    "    # 단어(토큰) 가중치 계산 및 범위 내 포함 단어(토큰) 추출\n",
    "    keywords = get_keywords(word_list)\n",
    "    \n",
    "    # 문장별 가중치 계산\n",
    "    sentence_list = get_sentences(content)\n",
    "    sentence_weight = []\n",
    "    \n",
    "    for sen in sentence_list:\n",
    "        sentence_weight.append((get_sentence_weight(sen, keywords), sen))\n",
    "           \n",
    "    # 문장별 가중치 역순 계산\n",
    "    sentence_weight.sort(reverse=True)\n",
    "#     print(sentence_weight)\n",
    "    \n",
    "    return [weight[1] for weight in sentence_weight[:max_no_of_sentences]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "aborted",
     "timestamp": 1625181451698,
     "user": {
      "displayName": "홍경수",
      "photoUrl": "",
      "userId": "10514684854108920109"
     },
     "user_tz": -540
    },
    "id": "mmZxMoce6S3E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) 의료비 절감 헬스케어 과 기업의 매출 향상을 통한 산업 육성 통신 산림 등도 눈길을 끈다.\n",
      "\n",
      "\n",
      "2) 22일 과학기술정보통신부는 서울 중구 대한상공회의소에서 데이터 생태계 조성과 혁신 성장의 기반 마련을 위한 빅데이터 플랫폼 및 센터 출범식 행사를 개최했다.\n",
      "\n",
      "\n",
      "3) 대상 분야는 금융 BC카드 환경 한국수자원공사 문화 한국문화정보원 교통 한국교통연구원 헬스케어 국립암센터 유통 소비 매일방송 통신 KT 중소기업 더존비즈온 지역경제 경기도청 산림 한국임업진흥원 등으로 현재 1차 공모를 통해 72개 빅데이터 센터를 선정했고 다음달 8일까지 2차 공모를 통해 28개를 추가 선정해 총 100개를 지원 운영할 계획이다.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "li = summarize (doc ,  3)\n",
    "for i, s in enumerate(li) :\n",
    "    print(f\"{i+1})\", s)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctQ8vU85d2Uy"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUOOmfhngBnb"
   },
   "source": [
    "# 9-2 Textrank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2cG-zDxW0aW"
   },
   "source": [
    "![대체 텍스트](https://www.researchgate.net/profile/Khushboo_Thakkar3/publication/232645575/figure/fig1/AS:575720050573312@1514273764062/Sample-graph-build-for-sentence-extraction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-wrC-1D_a0V"
   },
   "source": [
    "## 2.1 TextRank 직접 구현하기 (Matrix 활용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LJjhauc6Jlz"
   },
   "source": [
    "### 1) 자카드 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "aborted",
     "timestamp": 1625181451698,
     "user": {
      "displayName": "홍경수",
      "photoUrl": "",
      "userId": "10514684854108920109"
     },
     "user_tz": -540
    },
    "id": "yu3gn_zOE-fv"
   },
   "outputs": [],
   "source": [
    "Text = \"딸기 바나나 사과 파인애플. 바나나 사과 딸기 포도. 복숭아 수박. 파인애플 사과 딸기 바나나.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "aborted",
     "timestamp": 1625181451699,
     "user": {
      "displayName": "홍경수",
      "photoUrl": "",
      "userId": "10514684854108920109"
     },
     "user_tz": -540
    },
    "id": "ev4s4s0YBxa3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "# 문장간 유사도 측정 (자카드 유사도 사용)\n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    sentence1 = [token[0] for token in mecab.pos(sentence1) if token[1][0] in [\"N\",\"V\"]]\n",
    "    sentence2 = [token[0] for token in mecab.pos(sentence2) if token[1][0] in [\"N\",\"V\"]]\n",
    "    union = set(sentence1) | set(sentence2)\n",
    "    intersection = set(sentence1) & set(sentence2)\n",
    "    \n",
    "    return len(intersection)/len(union)\n",
    "\n",
    "sentence_similarity('나는 치킨을 좋아해','나는 치킨을 싫어해')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ma25LRfEx67"
   },
   "source": [
    "### 2) 그래프 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1625181451700,
     "user": {
      "displayName": "홍경수",
      "photoUrl": "",
      "userId": "10514684854108920109"
     },
     "user_tz": -540
    },
    "id": "38ThXVuHB2bA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.3380282 , 0.1690141 , 0.49295774],\n",
       "       [0.41379312, 0.        , 0.10344828, 0.48275864],\n",
       "       [0.5       , 0.25      , 0.        , 0.25      ],\n",
       "       [0.5072464 , 0.4057971 , 0.08695652, 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def buildMatrix(sentences):\n",
    "    score = np.ones(len(sentences), dtype=np.float32)\n",
    "    weighted_edge = np.zeros((len(sentences),len(sentences)), dtype=np.float32)\n",
    "    \n",
    "    # 문장별로 그래프 edge를 Matrix 형태로 생성\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            weighted_edge[i][j] = sentence_similarity(sentences[i], sentences[j])\n",
    "\n",
    "    # normalize \n",
    "    for i in range(len(weighted_edge)):\n",
    "        score[i] = weighted_edge[i].sum()\n",
    "        weighted_edge[i] /= score[i]\n",
    "     \n",
    "    return weighted_edge\n",
    "\n",
    "Text = \"딸기 바나나 사과 파인애플 수박. 바나나 사과 딸기 포도. 복숭아 수박. 파인애플 사과 딸기 바나나.\"\n",
    "buildMatrix(sent_tokenize(Text))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfYbGdLp6wrE"
   },
   "source": [
    "### 3) 문장 중요도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1625181451700,
     "user": {
      "displayName": "홍경수",
      "photoUrl": "",
      "userId": "10514684854108920109"
     },
     "user_tz": -540
    },
    "id": "FlAJ0nVaB36Z"
   },
   "outputs": [],
   "source": [
    "def scoring(weighted_edge, score, threshold=0.0001, d=0.85, max_iter = 50):\n",
    "    for iter in range(max_iter):\n",
    "        new_score = (1-d) * d * weighted_edge.T.dot(score)\n",
    "        if abs(new_score - score).sum() <= threshold:\n",
    "            break\n",
    "            \n",
    "        score = new_score\n",
    "        \n",
    "    return new_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJBLc2jG60OR"
   },
   "source": [
    "### 4) 문서 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1625181451701,
     "user": {
      "displayName": "홍경수",
      "photoUrl": "",
      "userId": "10514684854108920109"
     },
     "user_tz": -540
    },
    "id": "SkIdmjkNBZfz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ryleyun/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1625181451701,
     "user": {
      "displayName": "홍경수",
      "photoUrl": "",
      "userId": "10514684854108920109"
     },
     "user_tz": -540
    },
    "id": "1DVvQdjSB5PE"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def summarize(text, n=10):\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    weighted_edge = buildMatrix(sentences)\n",
    "    init_score = np.ones(len(sentences), dtype=np.float32)\n",
    "    score = scoring(weighted_edge, init_score)\n",
    "    \n",
    "    sorted_score = sorted(enumerate(score), key = lambda x: x[1], reverse=True)[:n]\n",
    "    \n",
    "    return [sentences[idx] for idx, sent in sorted_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1625181451702,
     "user": {
      "displayName": "홍경수",
      "photoUrl": "",
      "userId": "10514684854108920109"
     },
     "user_tz": -540
    },
    "id": "EqjLqMOyR18k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "딸기 바나나 사과 파인애플 수박.\n",
      "파인애플 사과 딸기 바나나.\n",
      "바나나 사과 딸기 포도.\n"
     ]
    }
   ],
   "source": [
    "summary = summarize(Text, 3)\n",
    "\n",
    "for sent in summary :\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J16o9Y89WgNT"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8Yj0ioJimhl"
   },
   "source": [
    "## 2.2 TextRank 직접 구현하기 (Graph 활용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1625181451702,
     "user": {
      "displayName": "홍경수",
      "photoUrl": "",
      "userId": "10514684854108920109"
     },
     "user_tz": -540
    },
    "id": "I9kctoHX8RJm"
   },
   "outputs": [],
   "source": [
    "Text = \"딸기 바나나 사과 딸기 파인애플. 바나나 사과 딸기. 복숭아 파인애플. 파인애플 사과 딸기 바나나.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-1Rfaqx7DFN"
   },
   "source": [
    "### 2) 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1625181451703,
     "user": {
      "displayName": "홍경수",
      "photoUrl": "",
      "userId": "10514684854108920109"
     },
     "user_tz": -540
    },
    "id": "yWefbGlx8W3-"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1625181451703,
     "user": {
      "displayName": "홍경수",
      "photoUrl": "",
      "userId": "10514684854108920109"
     },
     "user_tz": -540
    },
    "id": "wUAak7ej8IVp"
   },
   "outputs": [],
   "source": [
    "def sentences(text):\n",
    "    return sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTtB08RJ8Ixa"
   },
   "source": [
    "### 3) 자카드 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1625181451704,
     "user": {
      "displayName": "홍경수",
      "photoUrl": "",
      "userId": "10514684854108920109"
     },
     "user_tz": -540
    },
    "id": "vU3qw-rBpiOc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "# 문장간 유사도 측정 (자카드 유사도 사용)\n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    mecab = Mecab()\n",
    "    sent1 = mecab.morphs(sentence1)\n",
    "    sent2 = mecab.morphs(sentence2)\n",
    "\n",
    "    return len(set(sent1)&set(sent2))/len(set(sent1)|set(sent2))\n",
    "\n",
    "sentence_similarity('나는 치킨을 좋아해','나는 치킨을 싫어해')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1625181451704,
     "user": {
      "displayName": "홍경수",
      "photoUrl": "",
      "userId": "10514684854108920109"
     },
     "user_tz": -540
    },
    "id": "zuOJL2EW-uII"
   },
   "outputs": [],
   "source": [
    "def connect(nodes):\n",
    "    return [(start, end, sentence_similarity(start, end)) for start in nodes for end in nodes if start is not end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbxnM_Cd7Fri"
   },
   "source": [
    "### 3) 그래프 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1625181451705,
     "user": {
      "displayName": "홍경수",
      "photoUrl": "",
      "userId": "10514684854108920109"
     },
     "user_tz": -540
    },
    "id": "rKEErtAU-tmj"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def rank(nodes,edges):\n",
    "    graph=nx.diamond_graph()\n",
    "    graph.clear() \n",
    "    graph.add_nodes_from(nodes)\n",
    "    graph.add_weighted_edges_from(edges)\n",
    "\n",
    "    return nx.pagerank(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1625181451705,
     "user": {
      "displayName": "홍경수",
      "photoUrl": "",
      "userId": "10514684854108920109"
     },
     "user_tz": -540
    },
    "id": "H9p3wVLLo7L4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysIrWFvG7LX4"
   },
   "source": [
    "### 4) 문서 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "aborted",
     "timestamp": 1625181451706,
     "user": {
      "displayName": "홍경수",
      "photoUrl": "",
      "userId": "10514684854108920109"
     },
     "user_tz": -540
    },
    "id": "6XQkxYkv-zxI"
   },
   "outputs": [],
   "source": [
    "def summarize(text,num_summaries=3):\n",
    "    nodes = sentences(text)\n",
    "    edges = connect(nodes)\n",
    "    scores = rank(nodes, edges)\n",
    "    \n",
    "    return sorted(scores.items(), key = lambda x: x[1], reverse=True)[:num_summaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "aborted",
     "timestamp": 1625181451707,
     "user": {
      "displayName": "홍경수",
      "photoUrl": "",
      "userId": "10514684854108920109"
     },
     "user_tz": -540
    },
    "id": "UwlnlxyP8VM7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('딸기 바나나 사과 딸기 파인애플.', 0.3034803909925603)\n",
      "('파인애플 사과 딸기 바나나.', 0.3034803909925603)\n",
      "('바나나 사과 딸기.', 0.254517723082264)\n"
     ]
    }
   ],
   "source": [
    "summary=summarize(Text, 3)\n",
    "\n",
    "for sent in summary :\n",
    "  print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPayQbC4iqmB"
   },
   "source": [
    "## 2.3 gensim을 활용한 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "aborted",
     "timestamp": 1625181451707,
     "user": {
      "displayName": "홍경수",
      "photoUrl": "",
      "userId": "10514684854108920109"
     },
     "user_tz": -540
    },
    "id": "aLDc5qhTyXF3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['대상 분야는 금융 BC카드 환경 한국수자원공사 문화 한국문화정보원 교통 한국교통연구원 헬스케어 국립암센터 유통 소비 매일방송 통신 KT 중소기업 더존비즈온 지역경제 경기도청 산림 한국임업진흥원 등으로 현재 1차 공모를 통해 72개 빅데이터 센터를 선정했고 다음달 8일까지 2차 공모를 통해 28개를 추가 선정해 총 100개를 지원 운영할 계획이다.',\n",
       " '이를 통해 데이터 생태계를 혁신하고 기업의 경쟁력을 제고하는 역할을 수행한다.',\n",
       " 'AI 알고리즘 제공도센터는 우선 분야별 데이터 부족 문제를 해소하기 위해 올해 말까지 시장 수요가 높은 1400여종 신규 데이터를 생산 구축하고 사업이 완료되는 2021년까지 총 5100여종 양질의 풍부한 데이터를 생산 구축해 시장에 공급할 계획이다.',\n",
       " '100개 센터에서 수집된 데이터를 융합 분석한 뒤 맞춤형 데이터 제작 등 양질의 데이터로 재생산하고 기업들이 필요로 하는 데이터를 원하는 형태로 즉시 활용할 수 있도록 제공할 계획이다.',\n",
       " '유영민 과기정통부 장관은 오늘 출범식은 대한민국이 데이터 강국으로 가기 위한 초석을 놓은 자리 라며 세계 주요국들보다 데이터 경제로 나아가는 발걸음이 다소 늦었지만 빅데이터 플랫폼과 센터를 지렛대로 우리나라의 낙후된 데이터 생태계를 혁신하고 기업의 경쟁력을 한 단계 제고할 수 있도록 정책적 역량을 집중하겠다 고 밝혔다.']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.summarization.summarizer import summarize\n",
    "summarize(doc).split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_1gyP1uX2z1"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "09 Prac. Document Summarization의 사본",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
