{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('비타', 'NNP'), ('500', 'SN')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab.pos(\"비타500\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLGhbEiOoAR7"
   },
   "source": [
    "# 텍스트 전처리 (Text Preprocessing)\n",
    "\n",
    "*   텍스트를 자연어 처리를 위해 용도에 맞도록 사전에 표준화 하는 작업\n",
    "*   텍스트 내 정보를 유지하고, 중복을 제거하여 분석 효율성을 높이기 위해 전처리를 수행\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E585k45HDx5E"
   },
   "source": [
    "### 1) 토큰화 (Tokenizing)\n",
    "* 텍스트를 자연어 처리를 위해 분리 하는 것을\n",
    "* 토큰화는 단어별로 분리하는 \"단어 토큰화(Word Tokenization)\"와 문장별로 분리하는 \"문장 토큰화(Sentence Tokenization)\"로 구분\n",
    "\n",
    "(이후 실습에서는 단어 토큰화를 \"토큰화\"로 통일하여 칭하도록 한다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "senwNSwgDzQc"
   },
   "source": [
    "### 2) 품사 부착(PoS Tagging)\n",
    "* 각 토큰에 품사 정보를 추가\n",
    "* 분석시에 불필요한 품사를 제거하거나 (예. 조사, 접속사 등) 필요한 품사를 필터링 하기 위해 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R15ri5czDyzc"
   },
   "source": [
    "### 3) 개체명 인식 (NER, Named Entity Recognition)\n",
    "* 각 토큰의 개체 구분(기관, 인물, 지역, 날짜 등) 태그를 부착\n",
    "* 텍스트가 무엇과 관련되어있는지 구분하기 위해 사용\n",
    "* 예를 들어, 과일의 apple과 기업의 apple을 구분하는 방법이 개체명 인식임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dfq99EkzD1Tk"
   },
   "source": [
    "### 4) 원형 복원 (Stemming & Lemmatization)\n",
    "* 각 토큰의 원형 복원을 함으로써 토큰을 표준화하여 불필요한 데이터 중복을 방지 (=단어의 수를 줄일수 있어 연산을 효율성을 높임)\n",
    "* 어간 추출(Stemming) : 품사를 무시하고 규칙에 기반하여 어간을 추출\n",
    "* 표제어 추출 (Lemmatization) : 품사정보를 유지하여 표제어 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5HQOjRvDxmd"
   },
   "source": [
    "### 5) 불용어 처리 (Stopword)\n",
    "* 자연어 처리를 위해 불필요한 요소를 제거하는 작업\n",
    "* 불필요한 품사를 제거하는 작업과 불필요한 단어를 제거하는 작업으로 구성\n",
    "* 불필요한 토큰을 제거함으로써 연산의 효율성을 높임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaIYJczuaS0n"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KysKAL3VlgQN"
   },
   "source": [
    "# 1 영문 전처리 실습\n",
    "\n",
    "\n",
    "NLTK lib (https://www.nltk.org/) 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenining -> tagging -> chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mND0us3Jppcu"
   },
   "source": [
    "## 1.1 실습용 영문기사 수집\n",
    "온라인 기사를 바로 수집하여 실습데이터로 사용\n",
    "\n",
    "https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lB4TzdVQHAsN"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/\"\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = soup.select(\"p\")\n",
    "text = article[3].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yv0ASXb8qa6H"
   },
   "source": [
    "## 1.2 영문 토큰화\n",
    "https://www.nltk.org/api/nltk.tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ryleyun/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(\"Good muffins cost $3.88 in New York\")\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'yes', ',', 'she', 'does', 'mean', 'everybody', \"'\", 's', 'job', 'from', 'yours', 'to', 'mine', 'and', 'onward', 'to', 'the', 'role', 'of', 'grain', 'farmers', 'in', 'Egypt', ',', 'pastry', 'chefs', 'in', 'Paris', 'and', 'dog', 'walkers', 'in', 'Oregon', 'i', '.', 'e', '.', 'every', 'job', '.', 'We', 'will', 'now', 'be', 'able', 'to', 'help', 'direct', 'all', 'workers', '’', 'actions', 'and', 'behavior', 'with', 'a', 'new', 'degree', 'of', 'intelligence', 'that', 'comes', 'from', 'predictive', 'analytics', ',', 'all', 'stemming', 'from', 'the', 'AI', 'engines', 'we', 'will', 'now', 'increasingly', 'depend', 'upon', '.']\n"
     ]
    }
   ],
   "source": [
    "# wordpuncttokenizer는 puctution들도 모두 띄어서 나옴\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "word_punct = WordPunctTokenizer().tokenize(text)\n",
    "print(word_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'yes', ',', 'she', 'does', 'mean', 'everybody', \"'s\", 'job', 'from', 'yours', 'to', 'mine', 'and', 'onward', 'to', 'the', 'role', 'of', 'grain', 'farmers', 'in', 'Egypt', ',', 'pastry', 'chefs', 'in', 'Paris', 'and', 'dog', 'walkers', 'in', 'Oregon', 'i.e.', 'every', 'job.', 'We', 'will', 'now', 'be', 'able', 'to', 'help', 'direct', 'all', 'workers’', 'actions', 'and', 'behavior', 'with', 'a', 'new', 'degree', 'of', 'intelligence', 'that', 'comes', 'from', 'predictive', 'analytics', ',', 'all', 'stemming', 'from', 'the', 'AI', 'engines', 'we', 'will', 'now', 'increasingly', 'depend', 'upon', '.']\n"
     ]
    }
   ],
   "source": [
    "# TreebankWordTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "word_punct = TreebankWordTokenizer().tokenize(text)\n",
    "print(word_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - spaCy 라는 tokenizer도 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-Z-0Nnysqnq"
   },
   "source": [
    "## 1.3 영문 품사 부착 (PoS Tagging)\n",
    "분리한 토큰마다 품사를 부착한다\n",
    "\n",
    "https://www.nltk.org/api/nltk.tag.html\n",
    "\n",
    "태크목록 : https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ryleyun/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Good', 'JJ'), ('muffins', 'NNS'), ('cost', 'VBP'), ('$', '$'), ('3.88', 'CD'), ('in', 'IN'), ('New', 'NNP'), ('York', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "tagged = pos_tag(word_tokens)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDo-5-khs5Oz"
   },
   "source": [
    "## 1.4 개체명 인식 (NER, Named Entity Recognition)\n",
    "\n",
    "http://www.nltk.org/api/nltk.chunk.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/ryleyun/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/ryleyun/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"words\")\n",
    "nltk.download(\"maxent_ne_chunker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Good/JJ)\n",
      "  muffins/NNS\n",
      "  cost/VBP\n",
      "  $/$\n",
      "  3.88/CD\n",
      "  in/IN\n",
      "  (GPE New/NNP York/NNP))\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "ne_token = ne_chunk(tagged)\n",
    "print(ne_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHjV0h0ZtM-t"
   },
   "source": [
    "## 1.5 원형 복원\n",
    "각 토큰의 원형을 복원하여 표준화 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2eCnbChtXjo"
   },
   "source": [
    "### 1.5.1 어간추출 (Stemming)\n",
    "\n",
    "* 규칙에 기반 하여 토큰을 표준화\n",
    "* ning제거, ful 제거 등\n",
    "\n",
    "https://www.nltk.org/api/nltk.stem.html\n",
    "\n",
    "규칙상세 : https://tartarus.org/martin/PorterStemmer/def.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'believ'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"running\"); ps.stem(\"believers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4haNWIcCtZza"
   },
   "source": [
    "### 1.5.2 표제어 추출 (Lemmatization)\n",
    "\n",
    "* 품사정보를 보존하여 토큰을 표준화\n",
    "\n",
    "http://www.nltk.org/api/nltk.stem.html?highlight=lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ryleyun/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beautiful'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl.lemmatize(\"running\"); wl.lemmatize(\"beautiful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmY_SvDMb0fz"
   },
   "source": [
    "## 1.6 불용어 처리 (Stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', ','), 3),\n",
       " (('from', 'IN'), 3),\n",
       " (('to', 'TO'), 3),\n",
       " (('and', 'CC'), 3),\n",
       " (('in', 'IN'), 3)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_pos = ['IN', 'CC', 'DT']\n",
    "from collections import Counter\n",
    "tagged = pos_tag(word_tokenize(text))\n",
    "Counter(tagged).most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yes', ',', 'she', 'does', 'mean']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "for tag in tagged:\n",
    "    if not tag[1] in stop_pos:\n",
    "        words.append(tag[0])\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMErzPcbuYEa"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0Dhqm4zkHXl"
   },
   "source": [
    "# 2 한글 전처리 실습\n",
    "영문은 공백으로 토큰화가 가능하지만, 한글의 경우 품사를 고려하여 토큰화 해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIkGxDnNimek"
   },
   "source": [
    "## 2.1 실습용 한글기사 수집\n",
    "온라인 기사를 바로 수집하여 실습데이터로 사용\n",
    "\n",
    "http://news.chosun.com/site/data/html_dir/2018/07/10/2018071004121.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=110&oid=023&aid=0003386456\"\n",
    "headers = {\"user-agent\":\"Mozilla/5.0\"}\n",
    "res = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(res.text,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "article = soup.select_one(\"#articleBodyContents\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w09FHRgIphw5"
   },
   "source": [
    "## 2.2 한글 토큰화 및 형태소 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['암',\n",
       " '특이',\n",
       " '적',\n",
       " '바이오',\n",
       " '마커',\n",
       " '발굴',\n",
       " '및',\n",
       " '바이오',\n",
       " '마커',\n",
       " '에',\n",
       " '대한',\n",
       " '프로브',\n",
       " '개발',\n",
       " 'good']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "m = Mecab()\n",
    "tagged_m = m.morphs(\"암특이적바이오마커발굴및바이오마커에대한프로브개발good\")\n",
    "tagged_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Komoran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('암특이적바이오마커발굴및바이오마커에대한프로브개발', 'NA')]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Komoran\n",
    "k = Komoran()\n",
    "tagged_k = k.pos(\"암특이적바이오마커발굴및바이오마커에대한프로브개발\")\n",
    "tagged_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('암', 'Modifier'),\n",
       " ('특이', 'Noun'),\n",
       " ('적', 'Suffix'),\n",
       " ('바이오', 'Noun'),\n",
       " ('마커', 'Adverb'),\n",
       " ('발굴', 'Noun'),\n",
       " ('및', 'Noun'),\n",
       " ('바이오', 'Noun'),\n",
       " ('마커', 'Adverb'),\n",
       " ('에', 'Josa'),\n",
       " ('대', 'Modifier'),\n",
       " ('한프로', 'Noun'),\n",
       " ('브', 'Noun'),\n",
       " ('개발', 'Noun')]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "o = Okt()\n",
    "tagged_o = o.pos(\"암특이적바이오마커발굴및바이오마커에대한프로브개발\")\n",
    "tagged_o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kkma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('암', 'NNG'),\n",
       " ('특이', 'NNG'),\n",
       " ('적', 'XSN'),\n",
       " ('바이오', 'NNG'),\n",
       " ('마커', 'NNG'),\n",
       " ('발굴', 'NNG'),\n",
       " ('및', 'MAG'),\n",
       " ('바이오', 'NNG'),\n",
       " ('마커', 'NNG'),\n",
       " ('에', 'JKM'),\n",
       " ('대하', 'VV'),\n",
       " ('ㄴ', 'ETD'),\n",
       " ('프로브', 'NNG'),\n",
       " ('개발', 'NNG')]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "kk = Kkma()\n",
    "tagged_kk = kk.pos(\"암특이적바이오마커발굴및바이오마커에대한프로브개발\")\n",
    "tagged_kk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hannanum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('암특이적바이오마커발굴및바이오마커에대한프로브개발', 'N')]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Hannanum\n",
    "h = Hannanum()\n",
    "tagged_h = h.pos(\"암특이적바이오마커발굴및바이오마커에대한프로브개발\")\n",
    "tagged_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IZWN4xX4HXW"
   },
   "source": [
    "한글 자연어처리기 비교\n",
    "\n",
    "https://konlpy.org/ko/latest/morph/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - soynlp도 있다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2M7nyptjunTG"
   },
   "source": [
    "## 2.3 한글 품사 부착 (PoS Tagging)\n",
    "\n",
    "PoS Tag 목록\n",
    "\n",
    "https://docs.google.com/spreadsheets/u/1/d/1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit#gid=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('기존', 'NNG'),\n",
       " ('가속기', 'NNG'),\n",
       " ('설계', 'NNG'),\n",
       " ('및', 'MAJ'),\n",
       " ('운전', 'NNG'),\n",
       " ('에', 'JKB'),\n",
       " ('있', 'VV'),\n",
       " ('어서', 'EC'),\n",
       " ('엄밀', 'XR'),\n",
       " ('하', 'XSA'),\n",
       " ('게', 'EC'),\n",
       " ('취급', 'NNG'),\n",
       " ('되', 'VV'),\n",
       " ('지', 'EC'),\n",
       " ('않', 'VX'),\n",
       " ('았', 'EP'),\n",
       " ('던', 'ETM'),\n",
       " (',', 'SC'),\n",
       " ('공간', 'NNG'),\n",
       " ('전하', 'NNG'),\n",
       " ('효과', 'NNG'),\n",
       " ('(', 'SSO'),\n",
       " ('선형', 'NNG'),\n",
       " ('및', 'MAJ'),\n",
       " ('비', 'XPN'),\n",
       " ('선형', 'NNG'),\n",
       " (')', 'SSC'),\n",
       " (',', 'SC'),\n",
       " ('x', 'SL'),\n",
       " ('-', 'SY'),\n",
       " ('y', 'SL'),\n",
       " ('coupling', 'SL'),\n",
       " ('및', 'MAJ'),\n",
       " ('x', 'SL'),\n",
       " ('-', 'SY'),\n",
       " ('z', 'SL'),\n",
       " ('coupling', 'SL'),\n",
       " (',', 'SC'),\n",
       " ('급격', 'XR'),\n",
       " ('한', 'XSA+ETM'),\n",
       " ('에너지', 'NNG'),\n",
       " ('변화', 'NNG'),\n",
       " ('등', 'NNB'),\n",
       " ('을', 'JKO'),\n",
       " ('고려', 'NNG'),\n",
       " ('한', 'XSV+ETM'),\n",
       " ('빔', 'NNG'),\n",
       " ('물리', 'NNG'),\n",
       " ('이론', 'NNG'),\n",
       " ('모형', 'NNG'),\n",
       " ('을', 'JKO'),\n",
       " ('개발', 'NNG'),\n",
       " ('하', 'XSV'),\n",
       " ('고', 'EC'),\n",
       " (',', 'SC'),\n",
       " ('국제', 'NNG'),\n",
       " ('핵융합', 'NNG'),\n",
       " ('재료', 'NNG'),\n",
       " ('조사', 'NNG'),\n",
       " ('시설', 'NNG'),\n",
       " ('(', 'SSO'),\n",
       " ('IFMIF', 'SL'),\n",
       " (')', 'SSC'),\n",
       " ('개발', 'NNG'),\n",
       " ('에', 'JKB'),\n",
       " ('적용', 'NNG'),\n",
       " ('한다', 'XSV+EC')]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Komoran\n",
    "m = Mecab()\n",
    "pos_m = m.pos(\"기존 가속기 설계 및 운전에 있어서 엄밀하게 취급 되지 않았던, 공간전하 효과(선형 및 비선형), x-y coupling 및  x-z coupling, 급격한 에너지 변화 등을 고려한 빔 물리 이론 모형을 개발하고, 국제 핵융합 재료조사시설(IFMIF) 개발에 적용한다\")\n",
    "pos_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZY4s8tbuuXP"
   },
   "source": [
    "## 2.4 불용어(Stopword) 처리\n",
    "분석에 불필요한 품사를 제거하고, 불필요한 단어(불용어)를 제거한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_pos = ['EC','EP','SC', 'JK', 'JKO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for tag in pos_m:\n",
    "    if tag[1] in stop_pos:\n",
    "        continue\n",
    "    words.append(tag[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 56\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_m), len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1BDV2EAzug6"
   },
   "source": [
    "# 2 N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import bigrams, word_tokenize\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'a', 'boy']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I am a boy\"\n",
    "tokens = word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = bigrams(tokens)\n",
    "trigram = ngrams(tokens, 3, pad_right=True)\n",
    "# pad_right = True 하면 none과 함께 나옴\n",
    "# left_pad_symbol = \"\" -> 이것도 왼쪽 오른 쪽에 심볼도 넣을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'am')\n",
      "('am', 'a')\n",
      "('a', 'boy')\n",
      "('I', 'am', 'a')\n",
      "('am', 'a', 'boy')\n",
      "('a', 'boy', None)\n",
      "('boy', None, None)\n"
     ]
    }
   ],
   "source": [
    "for token in bigram:\n",
    "    print(token)\n",
    "    \n",
    "for token in trigram:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'am'), ('am', 'a'), ('a', 'boy')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigram)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "05 KS Practice. Text Preprocessing",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
