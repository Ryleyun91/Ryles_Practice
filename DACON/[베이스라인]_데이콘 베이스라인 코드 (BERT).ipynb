{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. 데이터 및 라이브러리 불러오기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고 코드: 텐서플로2와 머신러닝으로 시작하는 자연어처리(위키북스)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/NLP-kr/tensorflow-ml-nlp-tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score,f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')\n",
    "sample_submission=pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape:(174304, 13)\n",
      "test.shape:(43576, 12)\n",
      "train labels 개수: 46\n"
     ]
    }
   ],
   "source": [
    "print(f'train.shape:{train.shape}')\n",
    "print(f'test.shape:{test.shape}')\n",
    "print(f'train label 개수: {train.label.nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. 데이터 전처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dacon/.local/lib/python3.6/site-packages/pandas/core/series.py:4536: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  downcast=downcast,\n"
     ]
    }
   ],
   "source": [
    "#이번 베이스라인에서는 과제명 뿐만 아니라 요약문_연구내용도 모델에 학습시켜보겠습니다.\n",
    "train=train[['과제명', '요약문_연구내용','label']]\n",
    "test=test[['과제명', '요약문_연구내용']]\n",
    "train['요약문_연구내용'].fillna('NAN', inplace=True)\n",
    "test['요약문_연구내용'].fillna('NAN', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dacon/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "train['data']=train['과제명']+train['요약문_연구내용']\n",
    "test['data']=test['과제명']+test['요약문_연구내용']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174304, 4)\n",
      "(43576, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>과제명</th>\n",
       "      <th>요약문_연구내용</th>\n",
       "      <th>label</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>유전정보를 활용한 새로운 해충 분류군 동정기술 개발</td>\n",
       "      <td>(가) 외래 및 돌발해충의 발생조사 및 종 동정\\n\\n\\n    ○ 대상해충 : 최...</td>\n",
       "      <td>24</td>\n",
       "      <td>유전정보를 활용한 새로운 해충 분류군 동정기술 개발(가) 외래 및 돌발해충의 발생조...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>대장암의 TRAIL 내성 표적 인자 발굴 및 TRAIL 반응 예측 유전자 지도 구축...</td>\n",
       "      <td>1차년도\\n1) Microarray를 통한 선천적 TRAIL 내성 표적 후보 유전자...</td>\n",
       "      <td>0</td>\n",
       "      <td>대장암의 TRAIL 내성 표적 인자 발굴 및 TRAIL 반응 예측 유전자 지도 구축...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 과제명  \\\n",
       "0                       유전정보를 활용한 새로운 해충 분류군 동정기술 개발   \n",
       "1  대장암의 TRAIL 내성 표적 인자 발굴 및 TRAIL 반응 예측 유전자 지도 구축...   \n",
       "\n",
       "                                            요약문_연구내용  label  \\\n",
       "0  (가) 외래 및 돌발해충의 발생조사 및 종 동정\\n\\n\\n    ○ 대상해충 : 최...     24   \n",
       "1  1차년도\\n1) Microarray를 통한 선천적 TRAIL 내성 표적 후보 유전자...      0   \n",
       "\n",
       "                                                data  \n",
       "0  유전정보를 활용한 새로운 해충 분류군 동정기술 개발(가) 외래 및 돌발해충의 발생조...  \n",
       "1  대장암의 TRAIL 내성 표적 인자 발굴 및 TRAIL 반응 예측 유전자 지도 구축...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>과제명</th>\n",
       "      <th>요약문_연구내용</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R-FSSW 기술 적용 경량 차체 부품 개발 및 품질 평가를 위한 64채널 C-SC...</td>\n",
       "      <td>○ 1차년도\\n\\n    . 개발 탐촉 시스템의 성능 평가 위한 표준 시편 제작 시...</td>\n",
       "      <td>R-FSSW 기술 적용 경량 차체 부품 개발 및 품질 평가를 위한 64채널 C-SC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>다입자계를 묘사하는 편미분방정식에 대한 연구</td>\n",
       "      <td>연구과제1. 무한입자계의 동역학 / 작용소(operator) 방정식에 대한 연구\\n...</td>\n",
       "      <td>다입자계를 묘사하는 편미분방정식에 대한 연구연구과제1. 무한입자계의 동역학 / 작용...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 과제명  \\\n",
       "0  R-FSSW 기술 적용 경량 차체 부품 개발 및 품질 평가를 위한 64채널 C-SC...   \n",
       "1                           다입자계를 묘사하는 편미분방정식에 대한 연구   \n",
       "\n",
       "                                            요약문_연구내용  \\\n",
       "0  ○ 1차년도\\n\\n    . 개발 탐촉 시스템의 성능 평가 위한 표준 시편 제작 시...   \n",
       "1  연구과제1. 무한입자계의 동역학 / 작용소(operator) 방정식에 대한 연구\\n...   \n",
       "\n",
       "                                                data  \n",
       "0  R-FSSW 기술 적용 경량 차체 부품 개발 및 품질 평가를 위한 64채널 C-SC...  \n",
       "1  다입자계를 묘사하는 편미분방정식에 대한 연구연구과제1. 무한입자계의 동역학 / 작용...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. 모델링**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seed 고정\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 3\n",
    "VALID_SPLIT = 0.2\n",
    "MAX_LEN=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dacon/.local/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2111: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from transformers import *\n",
    "tokenizer=BertTokenizer.from_pretrained('bert-base-multilingual-cased',  cache_dir='bert_ckpt', do_lower_case=False)\n",
    "\n",
    "def bert_tokenizer(sent, MAX_LEN):\n",
    "    \n",
    "    encoded_dict=tokenizer.encode_plus(\n",
    "    text = sent, \n",
    "    add_special_tokens=True, \n",
    "    max_length=MAX_LEN, \n",
    "    pad_to_max_length=True, \n",
    "    return_attention_mask=True,\n",
    "    truncation = True)\n",
    "    \n",
    "    input_id=encoded_dict['input_ids']\n",
    "    attention_mask=encoded_dict['attention_mask']\n",
    "    token_type_id = encoded_dict['token_type_ids']\n",
    "    \n",
    "    return input_id, attention_mask, token_type_id\n",
    "\n",
    "input_ids =[]\n",
    "attention_masks =[]\n",
    "token_type_ids =[]\n",
    "train_data_labels = []\n",
    "\n",
    "def clean_text(sent):\n",
    "    sent_clean=re.sub(\"[^가-힣ㄱ-하-ㅣ]\", \" \", sent)\n",
    "    return sent_clean\n",
    "\n",
    "for train_sent, train_label in zip(train['data'], train['label']):\n",
    "    try:\n",
    "        input_id, attention_mask, token_type_id = bert_tokenizer(clean_text(train_sent), MAX_LEN=MAX_LEN)\n",
    "        \n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        #########################################\n",
    "        train_data_labels.append(train_label)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(train_sent)\n",
    "        pass\n",
    "\n",
    "train_input_ids=np.array(input_ids, dtype=int)\n",
    "train_attention_masks=np.array(attention_masks, dtype=int)\n",
    "train_token_type_ids=np.array(token_type_ids, dtype=int)\n",
    "###########################################################\n",
    "train_inputs=(train_input_ids, train_attention_masks, train_token_type_ids)\n",
    "train_labels=np.asarray(train_data_labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   101   9069  13890 119115  10459   8996  17138   9934  14801   9640\n",
      "  13764   9323 118654   9316   9321 119187   9576 119281   9625  16617\n",
      "  13764   9706  12092   8908  70122  10530  42300  91785   9730  10954\n",
      "  12092   9233   9879  11102   9428  38631  14801   8996  17138   9934\n",
      "  14801  10003  30005   9625  16617  13764   9428  61844   9069  13890\n",
      " 119115   9995  13764  20626  33077  10622   9638  61689  10003  30005\n",
      "   9625  16617  42984   9323  30842  11882   9323  30842   9543  14871\n",
      "   9367  40958  10003  30005   9625  16617  13764   9323  30842   9678\n",
      "  58931  10622   9638  65219   9934  14801   8843  74986  17138   9316\n",
      "   8996  17138   9672  12965   8932  16617   8922  16758   9934  14801\n",
      "   9625  16617  13764   9246  89108   8908  70122   9316   9095  29364\n",
      "  39420 118791  10622   9879  11102  10003  30005   9625  16617  13764\n",
      "   9934  14801   8843  74986  17138   8868 119230   9428  38631  14801\n",
      "   8996  17138   9576 119281  12030  13764   9323 118654   9730  10954\n",
      "  12092   9233   9879  11102  10003  38631  14801   8996  17138   9934\n",
      "  14801  10003  30005   9625  16617  13764   9428  61844   9069  13890\n",
      " 119115   9995  13764  20626  33077  10622   9638  61689  10003  30005\n",
      "   9625  16617  42984   9323  30842  11882   9323  30842   9543  14871\n",
      "   9367  40958  10003  30005   9625  16617  13764   9323  30842   9678\n",
      "  58931  10622   9638  65219   9934  14801   8843  74986  17138    102]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[CLS] 대장암의 내성 표적 인자 발굴 및 반응 예측 유전자 지도 구축에 관한 연구 차년도 를 통한 선천적 내성 표적 후보 유전자 선별 대장암 환자조직을 이용하여 후보 유전자의 발현과 발현 양상 분석 후보 유전자 발현 조절을 이용한 표적 가능성 및 내성 제어 기전 규명 표적 유전자 마우스 구축 및 동물모델을 통한 후보 유전자 표적 가능성 검증 선천적 내성 예측인자 발굴 차년도 를 통한 후천적 내성 표적 후보 유전자 선별 대장암 환자조직을 이용하여 후보 유전자의 발현과 발현 양상 분석 후보 유전자 발현 조절을 이용한 표적 가능성 [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(train_input_ids[1])\n",
    "print(train_attention_masks[1])\n",
    "print(train_token_type_ids[1])\n",
    "print(tokenizer.decode(train_input_ids[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e4d0b8a8cb46178b85af44d51d4985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b964e9e214f449ebee785da807a1ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf2_bert_classifier -- Folder already exists \n",
      "\n",
      "Epoch 1/30\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f1315aa1048>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7f13140d6048> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f1315aa1048>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7f13140d6048> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "4358/4358 [==============================] - ETA: 0s - loss: 0.7767 - accuracy: 0.8298WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "4358/4358 [==============================] - 1374s 312ms/step - loss: 0.7766 - accuracy: 0.8298 - val_loss: 0.5007 - val_accuracy: 0.8547\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.85465, saving model to tf2_bert_classifier/weights.h5\n",
      "Epoch 2/30\n",
      "4358/4358 [==============================] - 1362s 312ms/step - loss: 0.4176 - accuracy: 0.8752 - val_loss: 0.4047 - val_accuracy: 0.8804\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.85465 to 0.88041, saving model to tf2_bert_classifier/weights.h5\n",
      "Epoch 3/30\n",
      "4358/4358 [==============================] - 1362s 313ms/step - loss: 0.3247 - accuracy: 0.8992 - val_loss: 0.3749 - val_accuracy: 0.8908\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.88041 to 0.89079, saving model to tf2_bert_classifier/weights.h5\n",
      "Epoch 4/30\n",
      "4358/4358 [==============================] - 1362s 313ms/step - loss: 0.2644 - accuracy: 0.9155 - val_loss: 0.3709 - val_accuracy: 0.8993\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.89079 to 0.89934, saving model to tf2_bert_classifier/weights.h5\n",
      "Epoch 5/30\n",
      "4358/4358 [==============================] - 1362s 313ms/step - loss: 0.2158 - accuracy: 0.9311 - val_loss: 0.3674 - val_accuracy: 0.8949\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.89934\n",
      "Epoch 6/30\n",
      "4358/4358 [==============================] - 1362s 313ms/step - loss: 0.1799 - accuracy: 0.9421 - val_loss: 0.3722 - val_accuracy: 0.9011\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.89934 to 0.90106, saving model to tf2_bert_classifier/weights.h5\n",
      "Epoch 7/30\n",
      "4358/4358 [==============================] - 1362s 313ms/step - loss: 0.1511 - accuracy: 0.9508 - val_loss: 0.3848 - val_accuracy: 0.9061\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.90106 to 0.90606, saving model to tf2_bert_classifier/weights.h5\n",
      "Epoch 8/30\n",
      "4358/4358 [==============================] - 1362s 313ms/step - loss: 0.1287 - accuracy: 0.9585 - val_loss: 0.3896 - val_accuracy: 0.9071\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.90606 to 0.90706, saving model to tf2_bert_classifier/weights.h5\n",
      "Epoch 9/30\n",
      "4358/4358 [==============================] - 1362s 313ms/step - loss: 0.1103 - accuracy: 0.9651 - val_loss: 0.3989 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.90706\n",
      "Epoch 10/30\n",
      "4358/4358 [==============================] - 1361s 312ms/step - loss: 0.0992 - accuracy: 0.9685 - val_loss: 0.3865 - val_accuracy: 0.9124\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.90706 to 0.91237, saving model to tf2_bert_classifier/weights.h5\n",
      "Epoch 11/30\n",
      "4358/4358 [==============================] - 1362s 312ms/step - loss: 0.0913 - accuracy: 0.9711 - val_loss: 0.4199 - val_accuracy: 0.9108\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.91237\n",
      "Epoch 12/30\n",
      "4358/4358 [==============================] - 1354s 311ms/step - loss: 0.0835 - accuracy: 0.9736 - val_loss: 0.4307 - val_accuracy: 0.9037\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.91237\n",
      "Epoch 13/30\n",
      "4358/4358 [==============================] - 1357s 311ms/step - loss: 0.0784 - accuracy: 0.9744 - val_loss: 0.4304 - val_accuracy: 0.9116\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.91237\n",
      "Epoch 14/30\n",
      "4358/4358 [==============================] - 1349s 310ms/step - loss: 0.0744 - accuracy: 0.9758 - val_loss: 0.4371 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.91237 to 0.91561, saving model to tf2_bert_classifier/weights.h5\n",
      "Epoch 15/30\n",
      "4358/4358 [==============================] - 1365s 313ms/step - loss: 0.0705 - accuracy: 0.9779 - val_loss: 0.4498 - val_accuracy: 0.9102\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.91561\n",
      "Epoch 16/30\n",
      "4358/4358 [==============================] - 1364s 313ms/step - loss: 0.0657 - accuracy: 0.9798 - val_loss: 0.4168 - val_accuracy: 0.9139\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.91561\n",
      "Epoch 17/30\n",
      "4358/4358 [==============================] - 1351s 310ms/step - loss: 0.0635 - accuracy: 0.9799 - val_loss: 0.4741 - val_accuracy: 0.9065\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.91561\n",
      "Epoch 18/30\n",
      "4358/4358 [==============================] - 1352s 310ms/step - loss: 0.0603 - accuracy: 0.9808 - val_loss: 0.4931 - val_accuracy: 0.9168\n",
      "\n",
      "Epoch 00018: val_accuracy improved from 0.91561 to 0.91678, saving model to tf2_bert_classifier/weights.h5\n",
      "Epoch 19/30\n",
      "4358/4358 [==============================] - 1347s 309ms/step - loss: 0.0576 - accuracy: 0.9821 - val_loss: 0.4830 - val_accuracy: 0.9160\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.91678\n",
      "Epoch 20/30\n",
      "4358/4358 [==============================] - 1347s 309ms/step - loss: 0.0571 - accuracy: 0.9815 - val_loss: 0.4743 - val_accuracy: 0.9131\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.91678\n",
      "Epoch 21/30\n",
      "4358/4358 [==============================] - 1346s 309ms/step - loss: 0.0509 - accuracy: 0.9838 - val_loss: 0.4727 - val_accuracy: 0.9134\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.91678\n",
      "Epoch 22/30\n",
      "4358/4358 [==============================] - 1347s 309ms/step - loss: 0.0516 - accuracy: 0.9831 - val_loss: 0.4968 - val_accuracy: 0.9162\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.91678\n",
      "Epoch 23/30\n",
      "4358/4358 [==============================] - 1347s 309ms/step - loss: 0.0503 - accuracy: 0.9844 - val_loss: 0.4817 - val_accuracy: 0.9133\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.91678\n"
     ]
    }
   ],
   "source": [
    "class TFBertClassifier(tf.keras.Model):\n",
    "    def __init__(self, model_name, dir_path, num_class):\n",
    "        super(TFBertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)\n",
    "        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "        self.classifier = tf.keras.layers.Dense(num_class, \n",
    "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range), \n",
    "                                                name=\"classifier\")\n",
    "        \n",
    "    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n",
    "        \n",
    "        #outputs 값: # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs[1] \n",
    "        pooled_output = self.dropout(pooled_output, training=training)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return logits\n",
    "\n",
    "cls_model = TFBertClassifier(model_name='bert-base-multilingual-cased',\n",
    "                                  dir_path='bert_ckpt',\n",
    "                                  num_class=46)\n",
    "\n",
    "# 학습 준비하기\n",
    "optimizer = tf.keras.optimizers.Adam(3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "cls_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "model_name = \"tf2_bert_classifier\"\n",
    "\n",
    "# overfitting을 막기 위한 ealrystop 추가\n",
    "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001,patience=5)\n",
    "# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\n",
    "# patience: no improvment epochs (patience = 1, 1번 이상 상승이 없으면 종료)\\\n",
    "\n",
    "checkpoint_path = os.path.join(model_name, 'weights.h5')\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create path if exists\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
    "    \n",
    "cp_callback = ModelCheckpoint(\n",
    "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# 학습과 eval 시작\n",
    "history = cls_model.fit(train_inputs, train_labels, epochs=30, batch_size=32,\n",
    "                    validation_split = VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids =[]\n",
    "attention_masks =[]\n",
    "token_type_ids =[]\n",
    "train_data_labels = []\n",
    "\n",
    "def clean_text(sent):\n",
    "    sent_clean=re.sub(\"[^가-힣ㄱ-하-ㅣ]\", \" \", sent)\n",
    "    return sent_clean\n",
    "\n",
    "for test_sent in test['data']:\n",
    "    try:\n",
    "        input_id, attention_mask, token_type_id = bert_tokenizer(clean_text(test_sent), MAX_LEN=40)\n",
    "        \n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        #########################################\n",
    "       \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(test_sent)\n",
    "        pass\n",
    "    \n",
    "test_input_ids=np.array(input_ids, dtype=int)\n",
    "test_attention_masks=np.array(attention_masks, dtype=int)\n",
    "test_token_type_ids=np.array(token_type_ids, dtype=int)\n",
    "###########################################################\n",
    "test_inputs=(test_input_ids, test_attention_masks, test_token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "results = cls_model.predict(test_inputs)\n",
    "results=tf.argmax(results, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['label']=results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174304</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>174305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>174306</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>174307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>174308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43571</th>\n",
       "      <td>217875</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43572</th>\n",
       "      <td>217876</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43573</th>\n",
       "      <td>217877</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43574</th>\n",
       "      <td>217878</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43575</th>\n",
       "      <td>217879</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43576 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  label\n",
       "0      174304     18\n",
       "1      174305      0\n",
       "2      174306      0\n",
       "3      174307      0\n",
       "4      174308      0\n",
       "...       ...    ...\n",
       "43571  217875      0\n",
       "43572  217876      0\n",
       "43573  217877      2\n",
       "43574  217878      0\n",
       "43575  217879     19\n",
       "\n",
       "[43576 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('bert_baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "junyoung",
   "language": "python",
   "name": "junyoung"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
